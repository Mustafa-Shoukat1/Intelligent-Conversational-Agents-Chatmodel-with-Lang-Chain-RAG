{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283db80a",
   "metadata": {
    "papermill": {
     "duration": 0.014389,
     "end_time": "2024-08-14T18:53:25.142740",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.128351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius: 10px; border: 1px solid #000; padding: 10px; background: #fff; color: #000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);\">\n",
    "    <h1 style=\"color: #000; font-weight: bold; margin-bottom: 10px; font-size: 18px;\">\n",
    "        I comments most of code beacuse I don't want to reveal my API keys, so it‚Äôs not possible to run or make it public on Kaggle. It took me 10 days to develop, and I hope it helps you become an expert in RAG.\n",
    "    </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34f6b0",
   "metadata": {
    "papermill": {
     "duration": 0.012162,
     "end_time": "2024-08-14T18:53:25.167700",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.155538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview of RAG üß†‚ú®\n",
    "\n",
    "**Retrieval-augmented generation (RAG)** is an advanced approach in Natural Language Processing (NLP) that combines retrieval-based methods and generative models to provide more accurate and contextually relevant responses. \n",
    "\n",
    "### How RAG Works üõ†Ô∏è\n",
    "\n",
    "1. **Retrieval Step üîç**: Given a query, a set of relevant documents or passages is retrieved from a knowledge base.\n",
    "2. **Generation Step ‚úçÔ∏è**: The retrieved documents are then used as additional context to generate a response.\n",
    "\n",
    "### Advanced RAG üöÄ\n",
    "\n",
    "The advanced RAG model includes improvements in both the retrieval and generation processes to enhance the quality and relevance of the responses.\n",
    "\n",
    "#### Advanced Retrieval Techniques üåê\n",
    "\n",
    "1. **Dense Passage Retrieval (DPR) üèóÔ∏è**: Uses dense vector representations of passages and queries to improve retrieval accuracy.\n",
    "2. **Hybrid Retrieval üîÑ**: Combines both dense and sparse retrieval methods to balance precision and recall.\n",
    "3. **Context-Aware Retrieval üß©**: Adjusts retrieval strategies based on the context of the conversation or the user's history.\n",
    "\n",
    "#### Enhanced Generation ‚ú®\n",
    "\n",
    "1. **Fusion-in-Decoder (FiD) üõ†Ô∏è**: Integrates multiple retrieved passages in the decoder to generate more informed responses.\n",
    "2. **Cross-Attention Mechanisms üéØ**: Employs advanced attention mechanisms to focus on the most relevant parts of the retrieved documents.\n",
    "3. **Knowledge-Aware Generation üß†**: Incorporates structured knowledge (e.g., knowledge graphs) to ensure the generated content is factual and relevant.\n",
    "\n",
    "### Modular RAG üß©\n",
    "\n",
    "The modular RAG approach breaks down the RAG architecture into distinct, interchangeable components. This modularity allows for flexibility and easy upgrades of individual components without redesigning the entire system.\n",
    "\n",
    "#### Components of Modular RAG üõ†Ô∏è\n",
    "\n",
    "1. **Retrieval Module üîç**: Responsible for fetching relevant documents.\n",
    "   - **Indexer üìá**: Creates and maintains an index of the knowledge base.\n",
    "   - **Retriever üïµÔ∏è**: Searches the index to find relevant documents.\n",
    "2. **Generation Module ‚úçÔ∏è**: Generates the final response using the retrieved documents.\n",
    "   - **Encoder üéõÔ∏è**: Encodes the retrieved documents and the query.\n",
    "   - **Decoder üí°**: Generates the response using the encoded information.\n",
    "3. **Knowledge Base üìö**: The source of information for the retriever.\n",
    "   - **Static Knowledge Base üìñ**: Predefined set of documents.\n",
    "   - **Dynamic Knowledge Base üåê**: Continuously updated with new information.\n",
    "\n",
    "### Diagrams üìä\n",
    "\n",
    "#### Basic RAG Model\n",
    "\n",
    "```plaintext\n",
    "+------------+         +--------------+         +--------------+\n",
    "|   Query    |         |   Retriever  |         |  Generator   |\n",
    "+------------+   -->   +--------------+   -->   +--------------+\n",
    "                        /           \\            /            \\\n",
    "                       /             \\          /              \\\n",
    "                  +----+----+    +----+----+  +----+----+   +----+----+\n",
    "                  | Document |    | Document |  | Document |   | Document |\n",
    "                  +----+----+    +----+----+  +----+----+   +----+----+\n",
    "```\n",
    "\n",
    "#### Advanced RAG Model üöÄ\n",
    "\n",
    "```plaintext\n",
    "+------------+         +--------------+         +--------------+\n",
    "|   Query    |         |   Retriever  |         |  Generator   |\n",
    "+------------+   -->   +--------------+   -->   +--------------+\n",
    "                        /           \\            /            \\\n",
    "                       /             \\          /              \\\n",
    "                  +----+----+    +----+----+  +----+----+   +----+----+\n",
    "                  | Document |    | Document |  | Document |   | Document |\n",
    "                  +----+----+    +----+----+  +----+----+   +----+----+\n",
    "                     |              |             |              |\n",
    "                 +---+----+      +---+----+     +---+----+     +---+----+\n",
    "                 |   DPR  |      | Hybrid |     | Context|     | Fusion |\n",
    "                 |Retrieval|    |Retrieval|    |Aware Ret.|    |  in Decoder |\n",
    "                 +---------+     +---------+     +---------+     +---------+\n",
    "```\n",
    "\n",
    "#### Modular RAG Architecture üß©\n",
    "\n",
    "```plaintext\n",
    "+------------+          +-----------------+          +--------------+\n",
    "|   Query    |          |  Retrieval      |          |  Generation  |\n",
    "+------------+   -->    |   Module        |    -->   |    Module    |\n",
    "                        +-----------------+          +--------------+\n",
    "                             /      \\                     /   \\\n",
    "                            /        \\                   /     \\\n",
    "                   +------+           +------+   +------+     +------+\n",
    "                   |Index |           |Retriever|   | Encoder |   |Decoder|\n",
    "                   +------+           +------+   +------+     +------+\n",
    "```\n",
    "\n",
    "### Updated Information for 2024 üìÖ\n",
    "\n",
    "1. **Scalability üìà**: RAG models have been scaled to handle larger knowledge bases, making them more suitable for real-world applications.\n",
    "2. **Adaptability üîÑ**: Advances in transfer learning allow RAG models to adapt quickly to new domains with minimal additional training.\n",
    "3. **Multi-Language Support üåç**: Improved multilingual capabilities enable RAG models to support a broader range of languages.\n",
    "4. **Real-Time Updates ‚è±Ô∏è**: Integration with dynamic knowledge bases allows RAG models to incorporate the latest information in real-time.\n",
    "5. **User Personalization üë§**: Personalized retrieval and generation techniques enhance user-specific responses, improving user experience.\n",
    "\n",
    "By combining retrieval and generation in a sophisticated, modular framework, RAG models provide powerful tools for generating accurate and contextually relevant responses, making them invaluable for various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf2b6419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.194968Z",
     "iopub.status.busy": "2024-08-14T18:53:25.194573Z",
     "iopub.status.idle": "2024-08-14T18:53:25.200053Z",
     "shell.execute_reply": "2024-08-14T18:53:25.199050Z"
    },
    "id": "fnTLwrDuO4fB",
    "outputId": "19d1fd61-dcdc-450a-d417-1433e039c9f2",
    "papermill": {
     "duration": 0.022179,
     "end_time": "2024-08-14T18:53:25.202512",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.180333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere\n",
    "# # !pip install 'protobuf<=3.20.1' --force-reinstall\n",
    "# !python3 -m pip install pip --upgrade\n",
    "# !pip install pyopenssl --upgrade\n",
    "# # !pip install pymupdf\n",
    "# !pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711f5d7",
   "metadata": {
    "id": "9a5bed93",
    "papermill": {
     "duration": 0.012323,
     "end_time": "2024-08-14T18:53:25.227539",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.215216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### RAG exampls from scratch to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b927da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.255121Z",
     "iopub.status.busy": "2024-08-14T18:53:25.254731Z",
     "iopub.status.idle": "2024-08-14T18:53:25.260777Z",
     "shell.execute_reply": "2024-08-14T18:53:25.259688Z"
    },
    "id": "971b8eb2-54ca-4b16-b046-079d526b406e",
    "outputId": "60284d9a-473e-4a3a-da3e-b81a94756139",
    "papermill": {
     "duration": 0.022464,
     "end_time": "2024-08-14T18:53:25.263199",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.240735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import nltk\n",
    "# import bs4\n",
    "# from langchain import hub\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_community.llms import Ollama\n",
    "# from langchain_community.document_loaders import WebBaseLoader,ArxivLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_cohere import CohereEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_core.documents import Document\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # ! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain cohere\n",
    "# # # pip install 'protobuf<=3.20.1' --force-reinstall\n",
    "# # # !python3 -m pip install pip --upgrade\n",
    "# # # !pip install pyopenssl --upgrade\n",
    "# # !pip install pymupdf\n",
    "# # !pip install langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54162ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.291284Z",
     "iopub.status.busy": "2024-08-14T18:53:25.290004Z",
     "iopub.status.idle": "2024-08-14T18:53:25.295143Z",
     "shell.execute_reply": "2024-08-14T18:53:25.294121Z"
    },
    "id": "a66c5fd1-8da9-416f-b854-1b70d207d606",
    "papermill": {
     "duration": 0.021556,
     "end_time": "2024-08-14T18:53:25.297574",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.276018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# # os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
    "\n",
    "# os.environ['COHERE_API_KEY'] = \"\"\n",
    "# # os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedf25fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.325961Z",
     "iopub.status.busy": "2024-08-14T18:53:25.325014Z",
     "iopub.status.idle": "2024-08-14T18:53:25.330060Z",
     "shell.execute_reply": "2024-08-14T18:53:25.329009Z"
    },
    "id": "d28a4df7",
    "outputId": "d72c1f62-44fd-4a54-cb14-789f58198e08",
    "papermill": {
     "duration": 0.021801,
     "end_time": "2024-08-14T18:53:25.332558",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.310757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Paid\n",
    "# llm= ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# embeddings=OpenAIEmbeddings()\n",
    "\n",
    "### Free\n",
    "# from langchain.llms import GooglePalm\n",
    "\n",
    "# api_key = 'AIzaSyAW8XJLn0DnCLGueBlbpmFu5HMrTlkA_9E'\n",
    "# llm = GooglePalm(google_api_key=api_key, temperature=0.2)\n",
    "# print(llm('what is GenAI'))\n",
    "# embeddings=CohereEmbeddings()\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f5dd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.360738Z",
     "iopub.status.busy": "2024-08-14T18:53:25.360346Z",
     "iopub.status.idle": "2024-08-14T18:53:25.365116Z",
     "shell.execute_reply": "2024-08-14T18:53:25.364030Z"
    },
    "id": "4CCcOApsWnhd",
    "outputId": "4ca946bb-a5aa-4edd-8ce9-c225c0c0dabe",
    "papermill": {
     "duration": 0.021561,
     "end_time": "2024-08-14T18:53:25.367863",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.346302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install arxiv\n",
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e4fc05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.396398Z",
     "iopub.status.busy": "2024-08-14T18:53:25.395318Z",
     "iopub.status.idle": "2024-08-14T18:53:25.400602Z",
     "shell.execute_reply": "2024-08-14T18:53:25.399251Z"
    },
    "id": "b2B049nMrZwd",
    "papermill": {
     "duration": 0.02233,
     "end_time": "2024-08-14T18:53:25.403245",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.380915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb0b154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.430539Z",
     "iopub.status.busy": "2024-08-14T18:53:25.430123Z",
     "iopub.status.idle": "2024-08-14T18:53:25.436171Z",
     "shell.execute_reply": "2024-08-14T18:53:25.434958Z"
    },
    "id": "bea008f8",
    "outputId": "089bbc8d-a915-4a1f-96eb-cc7a6fb6b76f",
    "papermill": {
     "duration": 0.022673,
     "end_time": "2024-08-14T18:53:25.438733",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.416060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Post-processing\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# # Step 1: Load the documents\n",
    "# docs = ArxivLoader(query='2312.10997', load_max_docs=1, load_all_available_meta=True).load()\n",
    "# # print(docs)\n",
    "# load_docs = docs[0].page_content\n",
    "# # print(load_docs)\n",
    "\n",
    "# # Step 2: Filter complex metadata\n",
    "# filtered_docs = filter_complex_metadata(docs)\n",
    "\n",
    "# # Step 3: Split the documents\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "# splits = text_splitter.split_documents(filtered_docs)\n",
    "\n",
    "# # Step 4: Embed the documents\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# # Step 5: Create the retriever\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# # Prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# # Chain\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # Question\n",
    "# rag_chain.invoke(\"What is Modular RAG ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1bb1e",
   "metadata": {
    "id": "238ca125",
    "papermill": {
     "duration": 0.012722,
     "end_time": "2024-08-14T18:53:25.464433",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.451711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "How to write prompts- templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46518b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.493351Z",
     "iopub.status.busy": "2024-08-14T18:53:25.492930Z",
     "iopub.status.idle": "2024-08-14T18:53:25.498466Z",
     "shell.execute_reply": "2024-08-14T18:53:25.497183Z"
    },
    "id": "ba8ac6b7",
    "outputId": "5f1d2b53-12b9-497a-d62a-fb238b28ac8f",
    "papermill": {
     "duration": 0.022099,
     "end_time": "2024-08-14T18:53:25.501036",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.478937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prompt\n",
    "# template = \"\"\"You are a Q&A assistant, You will refer the context provided and answer the question.\n",
    "# If you dont know the answer , reply that you dont know the answer:\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbe27c",
   "metadata": {
    "id": "2e8fdb1d",
    "papermill": {
     "duration": 0.012434,
     "end_time": "2024-08-14T18:53:25.526195",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.513761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tweaking RAG : Work on queries, Work on prompts , work with rerankers & then see what you have got !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d609e6",
   "metadata": {
    "id": "a59482e5",
    "papermill": {
     "duration": 0.01207,
     "end_time": "2024-08-14T18:53:25.550764",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.538694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Multiquery Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d8a540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.577474Z",
     "iopub.status.busy": "2024-08-14T18:53:25.577050Z",
     "iopub.status.idle": "2024-08-14T18:53:25.583195Z",
     "shell.execute_reply": "2024-08-14T18:53:25.582135Z"
    },
    "id": "95e76bd0",
    "outputId": "3255fee3-459b-4c18-de76-397cc072f848",
    "papermill": {
     "duration": 0.022366,
     "end_time": "2024-08-14T18:53:25.585633",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.563267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Multi Query: Different Perspectives\n",
    "# template = \"\"\"You are an AI language model assistant.\n",
    "# Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector\n",
    "# database.\n",
    "# By generating multiple perspectives on the user question, your goal is to help\n",
    "# the user overcome some of the limitations of the distance-based similarity search.\n",
    "# Provide these alternative questions separated by newlines. Original question:\n",
    "# {question}\"\"\"\n",
    "\n",
    "# prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# generate_queries = (\n",
    "#     prompt_perspectives\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "#     | (lambda x: x.split(\"\\n\"))\n",
    "# )\n",
    "\n",
    "# from langchain.load import dumps, loads\n",
    "\n",
    "# def get_unique_union(documents: list[list]):\n",
    "#     \"\"\" Unique union of retrieved docs \"\"\"\n",
    "#     # Flatten list of lists, and convert each Document to string\n",
    "#     flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "#     # Get unique documents\n",
    "#     unique_docs = list(set(flattened_docs))\n",
    "#     # Return\n",
    "#     return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# # Retrieve\n",
    "# question = \"What is Modular RAG ?\"\n",
    "# retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "# docs = retrieval_chain.invoke({\"question\":question})\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab1c41a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.612391Z",
     "iopub.status.busy": "2024-08-14T18:53:25.611986Z",
     "iopub.status.idle": "2024-08-14T18:53:25.616992Z",
     "shell.execute_reply": "2024-08-14T18:53:25.615908Z"
    },
    "id": "8bcf9768",
    "outputId": "53a30bc8-5cf4-4ddf-8d66-86486fbb8f97",
    "papermill": {
     "duration": 0.021099,
     "end_time": "2024-08-14T18:53:25.619385",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.598286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # RAG\n",
    "# from operator import itemgetter\n",
    "\n",
    "# template = \"\"\"Answer the following question based on this context:\n",
    "# {context}\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# final_rag_chain = (\n",
    "#     {\"context\": retrieval_chain,\n",
    "#      \"question\": itemgetter(\"question\")}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4063",
   "metadata": {
    "id": "dac28af7",
    "papermill": {
     "duration": 0.012285,
     "end_time": "2024-08-14T18:53:25.644704",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.632419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " You can see our anwer depends on the answers which were coming out of the multiquery retriever chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35686c5",
   "metadata": {
    "id": "a0406721",
    "papermill": {
     "duration": 0.012297,
     "end_time": "2024-08-14T18:53:25.669595",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.657298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072acff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.696577Z",
     "iopub.status.busy": "2024-08-14T18:53:25.696114Z",
     "iopub.status.idle": "2024-08-14T18:53:25.701868Z",
     "shell.execute_reply": "2024-08-14T18:53:25.700713Z"
    },
    "id": "a367b583",
    "outputId": "e34ad887-40a2-4524-b92b-5048ca356258",
    "papermill": {
     "duration": 0.022099,
     "end_time": "2024-08-14T18:53:25.704470",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.682371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # Decomposition\n",
    "# template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "# The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "# Generate multiple search queries related to: {question} \\n\n",
    "# Output (3 queries):\"\"\"\n",
    "# prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "# # Chain\n",
    "# generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# # Run\n",
    "# question = \"I dont understand RAG ,Can you help me understand what are the components and one more thing I would like to know about whether is it same as Advanced RAG ?\"\n",
    "# #### I gave an ambigious query which talks about 3 questions 1. RAG understanding 2. Components of RAG 3. Difference between RAG & Advanced RAG\n",
    "# questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e0780d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.731619Z",
     "iopub.status.busy": "2024-08-14T18:53:25.731255Z",
     "iopub.status.idle": "2024-08-14T18:53:25.738027Z",
     "shell.execute_reply": "2024-08-14T18:53:25.736785Z"
    },
    "id": "5c175fdc",
    "outputId": "b34d2864-c9a3-4527-8952-85b1d0f4438d",
    "papermill": {
     "duration": 0.023398,
     "end_time": "2024-08-14T18:53:25.740715",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.717317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prompt\n",
    "# template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "# \\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "# Here is any available background question + answer pairs:\n",
    "\n",
    "# \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "# Here is additional context relevant to the question:\n",
    "\n",
    "# \\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "# Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "# \"\"\"\n",
    "\n",
    "# decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# from operator import itemgetter\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# def format_qa_pair(question, answer):\n",
    "#     \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "#     formatted_string = \"\"\n",
    "#     formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "#     return formatted_string.strip()\n",
    "\n",
    "\n",
    "\n",
    "# q_a_pairs = \"\"\n",
    "# for q in questions:\n",
    "\n",
    "#     rag_chain = (\n",
    "#     {\"context\": itemgetter(\"question\") | retriever,\n",
    "#      \"question\": itemgetter(\"question\"),\n",
    "#      \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "#     | decomposition_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser())\n",
    "\n",
    "#     answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "#     q_a_pair = format_qa_pair(q,answer)\n",
    "#     q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "\n",
    "# print(q_a_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b625a",
   "metadata": {
    "id": "cf4fa2d3",
    "papermill": {
     "duration": 0.012782,
     "end_time": "2024-08-14T18:53:25.766493",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.753711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It answered all the questions one by one Now its time to look at the actual query and revert it in a single answer.Lets print it and see what happens !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54a11834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.793491Z",
     "iopub.status.busy": "2024-08-14T18:53:25.793063Z",
     "iopub.status.idle": "2024-08-14T18:53:25.797662Z",
     "shell.execute_reply": "2024-08-14T18:53:25.796614Z"
    },
    "id": "d193e7fd",
    "outputId": "3167a9d2-f83f-4c83-9010-066f08856a26",
    "papermill": {
     "duration": 0.021017,
     "end_time": "2024-08-14T18:53:25.800226",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.779209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a832ed5",
   "metadata": {
    "id": "f1fc736b",
    "papermill": {
     "duration": 0.012467,
     "end_time": "2024-08-14T18:53:25.825565",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.813098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### HYDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ac8176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.852922Z",
     "iopub.status.busy": "2024-08-14T18:53:25.852499Z",
     "iopub.status.idle": "2024-08-14T18:53:25.858571Z",
     "shell.execute_reply": "2024-08-14T18:53:25.857413Z"
    },
    "id": "030681e7",
    "outputId": "6b4047e2-44ab-49c9-c0cd-f9ef862dad41",
    "papermill": {
     "duration": 0.022989,
     "end_time": "2024-08-14T18:53:25.861390",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.838401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # HyDE document genration\n",
    "# template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "# Question: {question}\n",
    "# Passage:\"\"\"\n",
    "# prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# generate_docs_for_retrieval = (\n",
    "#     prompt_hyde | llm | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # Run\n",
    "# generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "# # Retrieve\n",
    "# retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "# retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "# retireved_docs\n",
    "\n",
    "# # RAG\n",
    "# template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# final_rag_chain = (\n",
    "#     prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305ebba",
   "metadata": {
    "id": "9c3c1aa8",
    "papermill": {
     "duration": 0.012517,
     "end_time": "2024-08-14T18:53:25.886787",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.874270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1b70f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.913911Z",
     "iopub.status.busy": "2024-08-14T18:53:25.913545Z",
     "iopub.status.idle": "2024-08-14T18:53:25.921800Z",
     "shell.execute_reply": "2024-08-14T18:53:25.920742Z"
    },
    "id": "f201eec1-6ed0-4236-9594-286894574779",
    "outputId": "53fd4116-f980-48dc-8f89-fe1b7cf93bd0",
    "papermill": {
     "duration": 0.024713,
     "end_time": "2024-08-14T18:53:25.924221",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.899508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # RAG-Fusion\n",
    "# template = \"\"\"You are an assistant that generates multiple search queries based on a single input query. \\n\n",
    "# Generate multiple search queries related to: {question} \\n\n",
    "# Output (3 queries):\n",
    "# \"\"\"\n",
    "# prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# generate_queries = (\n",
    "#     prompt_rag_fusion\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "#     | (lambda x: x.split(\"\\n\"))\n",
    "# )\n",
    "\n",
    "# from langchain.load import dumps, loads\n",
    "\n",
    "# def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "#     \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "#         and an optional parameter k used in the RRF formula \"\"\"\n",
    "\n",
    "#     # Initialize a dictionary to hold fused scores for each unique document\n",
    "#     fused_scores = {}\n",
    "\n",
    "#     # Iterate through each list of ranked documents\n",
    "#     for docs in results:\n",
    "#         # Iterate through each document in the list, with its rank (position in the list)\n",
    "#         for rank, doc in enumerate(docs):\n",
    "#             # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "#             doc_str = dumps(doc)\n",
    "#             # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "#             if doc_str not in fused_scores:\n",
    "#                 fused_scores[doc_str] = 0\n",
    "#             # Retrieve the current score of the document, if any\n",
    "#             previous_score = fused_scores[doc_str]\n",
    "#             # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "#             fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "#     # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "#     reranked_results = [\n",
    "#         (loads(doc), score)\n",
    "#         for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "#     ]\n",
    "\n",
    "#     # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "#     return reranked_results\n",
    "\n",
    "# question = \"What is pattern in Modular RAG ?\"\n",
    "# retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "# docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "# print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e53fc56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:25.951315Z",
     "iopub.status.busy": "2024-08-14T18:53:25.950929Z",
     "iopub.status.idle": "2024-08-14T18:53:25.956442Z",
     "shell.execute_reply": "2024-08-14T18:53:25.955343Z"
    },
    "id": "ee43bb94-613d-46a3-b688-02c2d2aadfba",
    "outputId": "26a8ef5d-d8b9-4817-e96f-29c2059cb162",
    "papermill": {
     "duration": 0.022023,
     "end_time": "2024-08-14T18:53:25.958925",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.936902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# # RAG\n",
    "# template = \"\"\"\n",
    "# Answer the following question based on this context,\n",
    "# If you dont find any answer then just revert with 'Answer not found'.\n",
    "# context: {context}\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# #llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# final_rag_chain = (\n",
    "#     {\"context\": retrieval_chain_rag_fusion,\n",
    "#      \"question\": itemgetter(\"question\")}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11591d",
   "metadata": {
    "id": "3f21b53e",
    "papermill": {
     "duration": 0.012519,
     "end_time": "2024-08-14T18:53:25.984063",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.971544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Lets look at the Advanced RAG : Using CohereReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46975bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.011206Z",
     "iopub.status.busy": "2024-08-14T18:53:26.010786Z",
     "iopub.status.idle": "2024-08-14T18:53:26.016030Z",
     "shell.execute_reply": "2024-08-14T18:53:26.014916Z"
    },
    "id": "b88e1ad8-57eb-40d4-8475-a50775a692b0",
    "outputId": "a56f8938-c775-4c69-9846-0d1a9fd14edc",
    "papermill": {
     "duration": 0.021687,
     "end_time": "2024-08-14T18:53:26.018443",
     "exception": false,
     "start_time": "2024-08-14T18:53:25.996756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Cohere\n",
    "# from langchain.retrievers import  ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# # Chain\n",
    "# normal_rag_chain = (\n",
    "#     {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# # Question\n",
    "# normal_rag_chain.invoke(\"What is pattern in Modular RAG ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b10002d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.045984Z",
     "iopub.status.busy": "2024-08-14T18:53:26.045287Z",
     "iopub.status.idle": "2024-08-14T18:53:26.051174Z",
     "shell.execute_reply": "2024-08-14T18:53:26.050044Z"
    },
    "id": "637a669a",
    "outputId": "a6a61836-bc42-4513-c7da-5c009d4f78d0",
    "papermill": {
     "duration": 0.022378,
     "end_time": "2024-08-14T18:53:26.053680",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.031302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Re-rank\n",
    "# top_k=5\n",
    "# compressor = CohereRerank(top_n=top_k)\n",
    "# compression_retriever = ContextualCompressionRetriever(\n",
    "#     base_compressor=compressor, base_retriever=retriever\n",
    "# )\n",
    "# question=\"What is pattern in Modular RAG ?\"\n",
    "# compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "# #### After using reranker\n",
    "# reranked_rag_chain = (\n",
    "#     prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# print(\"Answer after reranking comes out to be: \")\n",
    "# print(reranked_rag_chain.invoke({\"context\":compressed_docs,\"question\":question}))\n",
    "\n",
    "\n",
    "# # The retrieved source documents\n",
    "# print(\"\\nRetrieved Documents:\")\n",
    "# for i in range(top_k):\n",
    "#     print(f\"\\nDocument {i+1}:\")\n",
    "#     print(compressed_docs[0].page_content)  # or doc.text depending on the document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd018bb",
   "metadata": {
    "id": "771bebec",
    "papermill": {
     "duration": 0.012285,
     "end_time": "2024-08-14T18:53:26.079053",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.066768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### You see how answer changes once you make use of additional retriever !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304cbff0",
   "metadata": {
    "id": "cf4b9175",
    "papermill": {
     "duration": 0.012404,
     "end_time": "2024-08-14T18:53:26.104276",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.091872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Advanced RAG Cohere Reranker(): How reranking looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b29b5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.132243Z",
     "iopub.status.busy": "2024-08-14T18:53:26.130906Z",
     "iopub.status.idle": "2024-08-14T18:53:26.136982Z",
     "shell.execute_reply": "2024-08-14T18:53:26.135902Z"
    },
    "id": "878fed01",
    "papermill": {
     "duration": 0.022518,
     "end_time": "2024-08-14T18:53:26.139444",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.116926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###### RAG & Applying Cohere Reranker for document extraction\n",
    "# import fitz  # PyMuPDF\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# # from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# # from langchain.llms import OpenAI\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "# from langchain import hub\n",
    "\n",
    "# # # Loads the latest version\n",
    "# # prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05f9dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.167478Z",
     "iopub.status.busy": "2024-08-14T18:53:26.167049Z",
     "iopub.status.idle": "2024-08-14T18:53:26.173864Z",
     "shell.execute_reply": "2024-08-14T18:53:26.172650Z"
    },
    "id": "aad9255a",
    "outputId": "4b19e919-8b3c-48d2-bde7-31961aeebe78",
    "papermill": {
     "duration": 0.02391,
     "end_time": "2024-08-14T18:53:26.176487",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.152577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Path to the PDF file\n",
    "\n",
    "# # Step 1: Load the documents\n",
    "# docs = ArxivLoader(query='2312.10997', load_max_docs=1, load_all_available_meta=True).load()\n",
    "# load_docs = docs[0].page_content\n",
    "\n",
    "# # Step 2: Filter complex metadata\n",
    "# data = filter_complex_metadata(docs)\n",
    "\n",
    "\n",
    "\n",
    "# # Split\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "# all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# # Store splits\n",
    "# # from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "# # Create a vector store with Chroma\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)\n",
    "\n",
    "# # RetrievalQA\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# # from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# # Set up the prompt template if needed\n",
    "# prompt_template = \"\"\"\n",
    "# Answer the following question based on the provided context.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# # Create the retriever with a specified top_k value\n",
    "# retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})  # Set top_k to 25\n",
    "\n",
    "# # Create the QA chain with the retriever and prompt\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt}, return_source_documents=True\n",
    "# )\n",
    "\n",
    "# # Run a query and see the results along with the context documents\n",
    "# query = \"Explain different components of Modular RAG?\"\n",
    "# result = qa_chain(query)\n",
    "\n",
    "# # The answer to the question\n",
    "# print(\"Answer:\", result['result'])\n",
    "\n",
    "# # The retrieved source documents\n",
    "# print(\"\\nRetrieved Documents:\")\n",
    "# for i, doc in enumerate(result['source_documents']):\n",
    "#     print(f\"\\nDocument {i+1}:\")\n",
    "#     print(doc.page_content)  # or doc.text depending on the document structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25eae424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.203994Z",
     "iopub.status.busy": "2024-08-14T18:53:26.203015Z",
     "iopub.status.idle": "2024-08-14T18:53:26.208617Z",
     "shell.execute_reply": "2024-08-14T18:53:26.207619Z"
    },
    "id": "a47df37f",
    "outputId": "98d8a3a4-0a69-4242-d61b-09cb9bf3ee7d",
    "papermill": {
     "duration": 0.021788,
     "end_time": "2024-08-14T18:53:26.211026",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.189238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Re-Rank them with cohere\n",
    "# import cohere\n",
    "# # Get your cohere API key on: www.cohere.com\n",
    "# co = cohere.Client(f\"{os.environ['COHERE_API_KEY']}\")\n",
    "# docs = [doc.page_content for doc in result['source_documents']]\n",
    "\n",
    "# # Re-Rank them with cohere\n",
    "# top_n=5\n",
    "# rerank_hits = co.rerank(query=query, documents=docs, top_n=top_n, model='rerank-multilingual-v3.0')\n",
    "# print(rerank_hits)\n",
    "# #[doc[rerank_hits.results[i].index] for i in range(5)]\n",
    "\n",
    "# for i, doc in enumerate(docs):\n",
    "#     if i>top_n-1:\n",
    "#         break\n",
    "#     else:\n",
    "#         print(f\"\\nDocument {i}:\")\n",
    "#         print(f\"Relevance score on the basis of reranking is : {rerank_hits.results[i].relevance_score}\")\n",
    "#         print(docs[rerank_hits.results[i].index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd77feb",
   "metadata": {
    "id": "pAuh9Fy14-Ii",
    "papermill": {
     "duration": 0.012129,
     "end_time": "2024-08-14T18:53:26.235763",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.223634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Character Text Splitting\n",
    "\n",
    "**Text:**\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Fixed Limit:** 10 characters\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "1. \"The quick \"\n",
    "2. \"brown fox \"\n",
    "3. \"jumps over\"\n",
    "4. \" the lazy \"\n",
    "5. \"dog.\"\n",
    "```\n",
    "\n",
    "### Recursive Character Text Splitting\n",
    "\n",
    "**Text:**\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "\n",
    "**Fixed Limit:** 10 characters\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "1. \"The quick\"\n",
    "2. \"brown fox\"\n",
    "3. \"jumps\"\n",
    "4. \"over\"\n",
    "5. \"the lazy\"\n",
    "6. \"dog\"\n",
    "```\n",
    "\n",
    "### MarkdownTextSplitter\n",
    "\n",
    "**Text:**\n",
    "```\n",
    "\"## Introduction\\n\\nThis is a sample markdown text with different sections and paragraphs.\\n\\n### Section 1\\n\\nSome content in section 1.\\n\\n### Section 2\\n\\nContent in section 2.\"\n",
    "```\n",
    "\n",
    "**Splitting Criteria:** Section headers (`##`, `###`)\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "1. \"## Introduction\\n\\nThis is a sample markdown text with different sections and paragraphs.\"\n",
    "2. \"### Section 1\\n\\nSome content in section 1.\"\n",
    "3. \"### Section 2\\n\\nContent in section 2.\"\n",
    "```\n",
    "\n",
    "### PythonCodeTextSplitter\n",
    "\n",
    "**Text:**\n",
    "```\n",
    "\"def calculate_sum(a, b):\\n    return a + b\\n\\ndef calculate_product(a, b):\\n    return a * b\\n\\nresult_sum = calculate_sum(3, 5)\\nresult_product = calculate_product(3, 5)\\nprint(result_sum)\\nprint(result_product)\"\n",
    "```\n",
    "\n",
    "**Splitting Criteria:** Function definitions (`def`)\n",
    "\n",
    "**Result:**\n",
    "```\n",
    "1. \"def calculate_sum(a, b):\\n    return a + b\\n\"\n",
    "2. \"def calculate_product(a, b):\\n    return a * b\\n\"\n",
    "3. \"result_sum = calculate_sum(3, 5)\\nresult_product = calculate_product(3, 5)\\nprint(result_sum)\\nprint(result_product)\"\n",
    "```\n",
    "\n",
    "### Semantic Chunking\n",
    "\n",
    "Semantic chunking groups text based on meaning or semantic units rather than characters or specific syntax. Example scenarios could involve grouping sentences based on topics or intents, which is more complex to demonstrate in a simple example.\n",
    "\n",
    "\n",
    "Here are the short definitions:\n",
    "\n",
    "### Character Text Splitting\n",
    "Breaks text based on a fixed number of characters.\n",
    "\n",
    "### Recursive Character Text Splitting\n",
    "Breaks text respecting natural language boundaries and recursively if needed.\n",
    "\n",
    "### MarkdownTextSplitter\n",
    "Splits Markdown text based on Markdown syntax like headers.\n",
    "\n",
    "### PythonCodeTextSplitter\n",
    "Divides Python code into segments based on syntactic units such as functions.\n",
    "\n",
    "### Semantic Chunking\n",
    "Groups text based on meaning or semantic units rather than specific syntax or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd1987",
   "metadata": {
    "id": "b6c2bdc4",
    "papermill": {
     "duration": 0.012134,
     "end_time": "2024-08-14T18:53:26.261105",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.248971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "###  Splitting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3babd00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.288207Z",
     "iopub.status.busy": "2024-08-14T18:53:26.287750Z",
     "iopub.status.idle": "2024-08-14T18:53:26.294139Z",
     "shell.execute_reply": "2024-08-14T18:53:26.293008Z"
    },
    "id": "20a2dd70",
    "outputId": "36ad7237-36a8-43a4-82b3-33d41d9ce287",
    "papermill": {
     "duration": 0.0227,
     "end_time": "2024-08-14T18:53:26.296506",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.273806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 1. Character Text Splitting\n",
    "# print(\"#### Character Text Splitting ####\")\n",
    "\n",
    "# text = \"\"\"In 2024, Pakistan will be at the epicenter of global attention, hosting two momentous events:\n",
    "#  the highly anticipated general elections and the ICC Cricket World Cup. The general elections will witness millions\n",
    "#  of eligible voters participating in a democratic exercise of unprecedented scale. Political parties are actively engaging their\n",
    "#   supporters with campaigns focused on pressing issues such as economic development, social equality, and national security.\n",
    "#   Concurrently, the nation will be swept up in cricket euphoria as teams from across the globe vie for supremacy in the Cricket World Cup.\n",
    "#   Cricket stadiums will reverberate with the enthusiastic cheers of fans, and the cricket pitches will witness thrilling displays of talent and sportsmanship.\n",
    "#   Amidst the fervor of political rallies and cricket matches, these parallel\n",
    "#  events will highlight Pakistan's unique national spirit‚Äîrooted in a steadfast commitment to democracy and an unwavering passion for cricket.\"\"\"\n",
    "# # Manual Splitting\n",
    "# chunks = []\n",
    "# chunk_size = 35 # Characters\n",
    "# for i in range(0, len(text), chunk_size):\n",
    "#     chunk = text[i:i + chunk_size]\n",
    "#     chunks.append(chunk)\n",
    "# documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
    "# print(documents)\n",
    "\n",
    "# # Automatic Text Splitting\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "# documents = text_splitter.create_documents([text])\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bb003a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.323418Z",
     "iopub.status.busy": "2024-08-14T18:53:26.322998Z",
     "iopub.status.idle": "2024-08-14T18:53:26.328417Z",
     "shell.execute_reply": "2024-08-14T18:53:26.327224Z"
    },
    "id": "204f9cab",
    "outputId": "eec06e8c-2819-473e-b9a5-85aad685a615",
    "papermill": {
     "duration": 0.021861,
     "end_time": "2024-08-14T18:53:26.331032",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.309171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 2. Recursive Character Text Splitting\n",
    "# print(\"#### Recursive Character Text Splitting ####\")\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
    "# print(text_splitter.create_documents([text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef633d1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-14T18:53:26.359220Z",
     "iopub.status.busy": "2024-08-14T18:53:26.358771Z",
     "iopub.status.idle": "2024-08-14T18:53:26.365343Z",
     "shell.execute_reply": "2024-08-14T18:53:26.363983Z"
    },
    "id": "21b15eea",
    "outputId": "9a0eba14-4ace-4326-bbd6-3c666f2629c1",
    "papermill": {
     "duration": 0.02385,
     "end_time": "2024-08-14T18:53:26.368222",
     "exception": false,
     "start_time": "2024-08-14T18:53:26.344372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 3. Document Specific Splitting\n",
    "# print(\"#### Document Specific Splitting ####\")\n",
    "\n",
    "# # Document Specific Splitting - Markdown\n",
    "# from langchain.text_splitter import MarkdownTextSplitter\n",
    "# splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "# markdown_text = text\n",
    "# print(splitter.create_documents([markdown_text]))\n",
    "\n",
    "# # Document Specific Splitting - Python\n",
    "# from langchain.text_splitter import PythonCodeTextSplitter\n",
    "# python_text = \"\"\"\n",
    "# class Person:\n",
    "#   def __init__(self, name, age):\n",
    "#     self.name = name\n",
    "#     self.age = age\n",
    "\n",
    "# p1 = Person(\"John\", 36)\n",
    "\n",
    "# for i in range(10):\n",
    "#     print (i)\n",
    "# \"\"\"\n",
    "# python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "# print(python_splitter.create_documents([python_text]))\n",
    "\n",
    "# # Document Specific Splitting - Javascript\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "# javascript_text = \"\"\"\n",
    "# // Function is called, the return value will end up in x\n",
    "# let x = myFunction(4, 3);\n",
    "\n",
    "# function myFunction(a, b) {\n",
    "# // Function returns the product of a and b\n",
    "#   return a * b;\n",
    "# }\n",
    "# \"\"\"\n",
    "# js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "#     language=Language.JS, chunk_size=65, chunk_overlap=0\n",
    "# )\n",
    "# print(js_splitter.create_documents([javascript_text]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.598635,
   "end_time": "2024-08-14T18:53:26.701752",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-14T18:53:22.103117",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
